{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import gc\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import lightgbm as lgb\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../input/bert_uncased_768')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Utils from Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-16 23:51:03--  https://raw.githubusercontent.com/google-research/bert/master/modeling.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 37922 (37K) [text/plain]\r\n",
      "Saving to: ‘modeling.py’\r\n",
      "\r\n",
      "modeling.py         100%[===================>]  37.03K  --.-KB/s    in 0.007s  \r\n",
      "\r\n",
      "2019-04-16 23:51:03 (5.10 MB/s) - ‘modeling.py’ saved [37922/37922]\r\n",
      "\r\n",
      "--2019-04-16 23:51:03--  https://raw.githubusercontent.com/google-research/bert/master/extract_features.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 13898 (14K) [text/plain]\r\n",
      "Saving to: ‘extract_features.py’\r\n",
      "\r\n",
      "extract_features.py 100%[===================>]  13.57K  --.-KB/s    in 0.007s  \r\n",
      "\r\n",
      "2019-04-16 23:51:04 (1.91 MB/s) - ‘extract_features.py’ saved [13898/13898]\r\n",
      "\r\n",
      "--2019-04-16 23:51:04--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 12257 (12K) [text/plain]\r\n",
      "Saving to: ‘tokenization.py’\r\n",
      "\r\n",
      "tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2019-04-16 23:51:04 (109 MB/s) - ‘tokenization.py’ saved [12257/12257]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import modeling\n",
    "import extract_features\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>True</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>True</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>False</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>False</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>False</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                        ...                                                                        URL\n",
       "0  development-1                        ...                          http://en.wikipedia.org/wiki/List_of_Teachers_...\n",
       "1  development-2                        ...                              http://en.wikipedia.org/wiki/Warren_MacKenzie\n",
       "2  development-3                        ...                          http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...\n",
       "3  development-4                        ...                                  http://en.wikipedia.org/wiki/Crime_(band)\n",
       "4  development-5                        ...                                http://en.wikipedia.org/wiki/Jessica_Rivera\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df  = pd.read_table('../input/gap-coreference/gap-development.tsv')\n",
    "train_df = pd.read_table('../input/gap-coreference/gap-test.tsv')\n",
    "val_df   = pd.read_table('../input/gap-coreference/gap-validation.tsv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to get distance embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is referenced from \n",
    "#https://www.kaggle.com/keyit92/coref-by-mlp-cnn-coattention\n",
    "\n",
    "def bs(lens, target):\n",
    "    low, high = 0, len(lens) - 1\n",
    "\n",
    "    while low < high:\n",
    "        mid = low + int((high - low) / 2)\n",
    "\n",
    "        if target > lens[mid]:\n",
    "            low = mid + 1\n",
    "        elif target < lens[mid]:\n",
    "            high = mid\n",
    "        else:\n",
    "            return mid + 1\n",
    "\n",
    "    return low\n",
    "\n",
    "def bin_distance(dist):\n",
    "    \n",
    "    buckets = [1, 2, 3, 4, 5, 8, 16, 32, 64]  \n",
    "    low, high = 0, len(buckets)\n",
    "    while low < high:\n",
    "        mid = low + int((high-low) / 2)\n",
    "        if dist > buckets[mid]:\n",
    "            low = mid + 1\n",
    "        elif dist < buckets[mid]:\n",
    "            high = mid\n",
    "        else:\n",
    "            return mid\n",
    "\n",
    "    return low\n",
    "\n",
    "def distance_features(P, A, B, char_offsetP, char_offsetA, char_offsetB, text, URL):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lens = [token.idx for token in doc]\n",
    "    mention_offsetP = bs(lens, char_offsetP) - 1\n",
    "    mention_offsetA = bs(lens, char_offsetA) - 1\n",
    "    mention_offsetB = bs(lens, char_offsetB) - 1\n",
    "    \n",
    "    mention_distA = mention_offsetP - mention_offsetA \n",
    "    mention_distB = mention_offsetP - mention_offsetB\n",
    "    \n",
    "    splited_A = A.split()[0].replace(\"*\", \"\")\n",
    "    splited_B = B.split()[0].replace(\"*\", \"\")\n",
    "    \n",
    "    if re.search(splited_A[0], str(URL)):\n",
    "        contains = 0\n",
    "    elif re.search(splited_B[0], str(URL)):\n",
    "        contains = 1\n",
    "    else:\n",
    "        contains = 2\n",
    "    \n",
    "    dist_binA = bin_distance(mention_distA)\n",
    "    dist_binB = bin_distance(mention_distB)\n",
    "    output =  [dist_binA, dist_binB, contains]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def extract_dist_features(df):\n",
    "    \n",
    "    index = df.index\n",
    "    columns = [\"D_PA\", \"D_PB\", \"IN_URL\"]\n",
    "    dist_df = pd.DataFrame(index = index, columns = columns)\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        \n",
    "        text = df.loc[i, 'Text']\n",
    "        P_offset = df.loc[i,'Pronoun-offset']\n",
    "        A_offset = df.loc[i, 'A-offset']\n",
    "        B_offset = df.loc[i, 'B-offset']\n",
    "        P, A, B  = df.loc[i,'Pronoun'], df.loc[i, 'A'], df.loc[i, 'B']\n",
    "        URL = df.loc[i, 'URL']\n",
    "        \n",
    "        dist_df.iloc[i] = distance_features(P, A, B, P_offset, A_offset, B_offset, text, URL)\n",
    "        \n",
    "    return dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the distance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e9ace58bcd4329acb59fb855f99006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33de3407c2744cf0ad13a0d00bc243a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=454), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfc36cc81b247ed96d9ce508f6cc181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dist_df = extract_dist_features(test_df)\n",
    "val_dist_df = extract_dist_features(val_df)\n",
    "train_dist_df = extract_dist_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the distance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dist_df.to_pickle('test_dist_df.pkl')\n",
    "val_dist_df.to_pickle('val_dist_df.pkl')\n",
    "train_dist_df.to_pickle('train_dist_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the distance features from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dist_df = pd.read_pickle('test_dist_df.pkl')\n",
    "val_dist_df = pd.read_pickle('val_dist_df.pkl')\n",
    "train_dist_df = pd.read_pickle('train_dist_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils to get BERT embeddings of A,B, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_char(text, offset):\n",
    "    \"Count the character numbers before the offset\"\n",
    "    count = 0\n",
    "    for pos in range(offset):\n",
    "        if text[pos] != \" \": count +=1\n",
    "    return count\n",
    "\n",
    "def candidate_length(candidate):\n",
    "    \"Calculate the length of the candidate A and B\"\n",
    "    count = 0\n",
    "    for i in range(len(candidate)):\n",
    "        if candidate[i] !=  \" \": count += 1\n",
    "    return count\n",
    "\n",
    "def count_token_length_special(token):\n",
    "    \"Calculate the non special token number\"\n",
    "    count = 0\n",
    "    special_token = [\"#\", \" \"]\n",
    "    for i in range(len(token)):\n",
    "        if token[i] not in special_token: count+=1\n",
    "    return count\n",
    "\n",
    "def embed_by_bert(df):\n",
    "    \"Run the pretrained bert model to get the embedding\"\n",
    "    \n",
    "    # Get the text feed to bert\n",
    "    text = df['Text']\n",
    "    text.to_csv('input.txt', index=False, header=False)\n",
    "    os.system(\"python3 extract_features.py \\\n",
    "               --input_file=input.txt \\\n",
    "               --output_file=output.jsonl \\\n",
    "               --vocab_file=../input/bert-uncased-768/vocab.txt \\\n",
    "               --bert_config_file=../input/bert-uncased-768/bert_config.json \\\n",
    "               --init_checkpoint=../input/bert-uncased-768/bert_model.ckpt \\\n",
    "               --layers=-1 \\\n",
    "               --max_seq_length=256 \\\n",
    "               --batch_size=8\")\n",
    "    \n",
    "    # BERT would generate the output.json1 file\n",
    "    bert_output = pd.read_json(\"output.jsonl\", lines = True)\n",
    "    bert_output.head()\n",
    "    \n",
    "    os.system(\"rm input.txt\")\n",
    "    os.system(\"rm output.jsonl\")\n",
    "    \n",
    "    index = df.index\n",
    "    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n",
    "    emb = pd.DataFrame(index = index, columns = columns)\n",
    "    emb.index.name = \"ID\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(len(text))):\n",
    "        \n",
    "        features = bert_output.loc[i, \"features\"]\n",
    "        # Get the character numbers before the Pronoun offset\n",
    "        P_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'Pronoun-offset'])\n",
    "        \n",
    "        # Get the character numbers before the A-offset\n",
    "        A_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'A-offset'])\n",
    "        \n",
    "        # Get the character numbers before the B-offset\n",
    "        B_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'B-offset'])\n",
    "        \n",
    "        # Get the lengths of those two names\n",
    "        A_length = candidate_length(df.loc[i, 'A'])\n",
    "        B_length = candidate_length(df.loc[i, 'B'])\n",
    "        \n",
    "        emb_A = np.zeros(768)\n",
    "        emb_B = np.zeros(768)\n",
    "        emb_P = np.zeros(768)\n",
    "        \n",
    "        char_count = 0\n",
    "        cnt_A, cnt_B = 0, 0\n",
    "        \n",
    "        for j in range(2, len(features)):\n",
    "            token = features[j][\"token\"]\n",
    "            token_length = count_token_length_special(token)\n",
    "            if char_count == P_char_start:\n",
    "                emb_P += np.asarray(features[j][\"layers\"][0]['values']) \n",
    "            if char_count in range(A_char_start, A_char_start+A_length):\n",
    "                emb_A += np.asarray(features[j][\"layers\"][0]['values'])\n",
    "                cnt_A += 1\n",
    "            if char_count in range(B_char_start, B_char_start+B_length):\n",
    "                emb_B += np.asarray(features[j][\"layers\"][0]['values'])\n",
    "                cnt_B += 1                \n",
    "            char_count += token_length\n",
    "        \n",
    "        emb_A /= cnt_A\n",
    "        emb_B /= cnt_B\n",
    "        \n",
    "        # Specify the label\n",
    "        label = \"Neither\"\n",
    "        if (df.loc[i,\"A-coref\"] == True):\n",
    "            label = \"A\"\n",
    "        if (df.loc[i,\"B-coref\"] == True):\n",
    "            label = \"B\"\n",
    "\n",
    "        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n",
    "        \n",
    "    return emb     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8029c5147548d7b9893d7a44b11426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:83: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec02c070f81c4ac9bb25451ebaaaa210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=454), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8176a354868148ec9dfeab2adf83c2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:83: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_emb = embed_by_bert(test_df)\n",
    "validation_emb = embed_by_bert(val_df)\n",
    "train_emb = embed_by_bert(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb.to_pickle(\"test_emb.pkl\")\n",
    "validation_emb.to_pickle(\"valid_emb.pkl\")\n",
    "train_emb.to_pickle(\"train_emb.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = pd.read_pickle('test_emb.pkl')\n",
    "validation_emb = pd.read_pickle('valid_emb.pkl')\n",
    "train_emb = pd.read_pickle('train_emb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the count features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_count_features(df):\n",
    "    index = df.index\n",
    "    columns = [\"cnt_A\", \"cnt_B\", \"cnt_P\"]\n",
    "    cnt_df = pd.DataFrame(index = index, columns = columns)\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        \n",
    "        text = df.loc[i, 'Text']\n",
    "        P, A, B  = df.loc[i,'Pronoun'], df.loc[i, 'A'], df.loc[i, 'B']\n",
    "        cnt_A = text.count(A)\n",
    "        cnt_B = text.count(B)\n",
    "        cnt_P = text.count(P)\n",
    "        \n",
    "        cnt_df.iloc[i] = cnt_A, cnt_B, cnt_P\n",
    "        \n",
    "    return cnt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7329e639804a10bcd0dbbca6de7383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034660c371684a8da97f7c91123235e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=454), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd181ad90ac47f2806d8115e4dd2057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_count_df = extract_count_features(test_df)\n",
    "val_count_df = extract_count_features(val_df)\n",
    "train_count_df = extract_count_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinism\n",
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=828)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_A</th>\n",
       "      <th>emb_B</th>\n",
       "      <th>emb_P</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.14526466666666668, -0.36192699999999994, 0...</td>\n",
       "      <td>[-0.47775049999999997, -0.598545, 0.426645, 0....</td>\n",
       "      <td>[-0.551684, -0.023438, -0.549022, -0.159332, 0...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.043327, 0.055112999999999995, 0.48952799999...</td>\n",
       "      <td>[0.02212816666666667, -0.43065516666666664, 0....</td>\n",
       "      <td>[0.06615499999999999, -0.050293, -0.173699, 0....</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.09385075000000001, 0.701532, 0.18457674999...</td>\n",
       "      <td>[0.227958, 0.531238, -0.43523599999999996, -0....</td>\n",
       "      <td>[-0.266816, 0.18423899999999999, -0.149993, -0...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.8687406666666666, -0.04495166666666665, 0....</td>\n",
       "      <td>[-0.381581, -0.103236, 0.6846015000000001, 0.0...</td>\n",
       "      <td>[0.100225, -0.19070199999999998, -0.4836149999...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.327636, -0.043740999999999995, 0.470923, 0....</td>\n",
       "      <td>[-0.5405355000000001, -0.7521034999999999, -0....</td>\n",
       "      <td>[-0.541744, 0.6634639999999999, 0.291974, 0.06...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                emb_A  ...  label\n",
       "ID                                                     ...       \n",
       "0   [-0.14526466666666668, -0.36192699999999994, 0...  ...      B\n",
       "1   [0.043327, 0.055112999999999995, 0.48952799999...  ...      A\n",
       "2   [-0.09385075000000001, 0.701532, 0.18457674999...  ...      A\n",
       "3   [-0.8687406666666666, -0.04495166666666665, 0....  ...      B\n",
       "4   [0.327636, -0.043740999999999995, 0.470923, 0....  ...      A\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = pd.concat([train_emb, validation_emb, test_emb]).reset_index(drop=True)\n",
    "dist = pd.concat([train_dist_df, val_dist_df, test_dist_df]).reset_index(drop=True)\n",
    "cnt = pd.concat([train_count_df, val_count_df, test_count_df]).reset_index(drop=True)\n",
    "dist = pd.concat([dist, cnt], axis = 1)\n",
    "train_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d1e42a97194ab4b83a815b1f490997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12359), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8e4abdb9a01b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_df\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/gendered-pronoun-resolution/test_stage_2.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_dist_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dist_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_by_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_count_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_count_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-342449b75bc1>\u001b[0m in \u001b[0;36membed_by_bert\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# BERT would generate the output.json1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mbert_output\u001b[0m\u001b[0;34m